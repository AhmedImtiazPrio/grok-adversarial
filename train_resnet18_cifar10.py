# -*- coding: utf-8 -*-
"""Deep Networks Always Grok : ResNet18-CIFAR10

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1flLHh99_0jnZh0Bf2o-keNbMpCOtB44C
"""

## Demo codes for "Deep Networks Always Grok and Here is Why"
## Authors: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk
## Website: bit.ly/grok-adversarial

import torch as ch
import torchvision
from torchvision import transforms
from torch.nn import CrossEntropyLoss
from torch.optim import SGD, lr_scheduler, AdamW
import numpy as np

import ml_collections
from tqdm import tqdm
import os
import time
import logging

import wandb


from dataloaders import cifar10_dataloaders, cifar10_dataloaders_ffcv, get_LC_samples
from models import make_resnet18k
from utils import flatten_model, add_hooks_preact_resnet18
from attacks import PGD
from local_complexity import get_intersections_for_hulls
from samplers import get_ortho_hull_around_samples, get_ortho_hull_around_samples_w_orig
from configs import config_resnet18_cifar10

#@title Train and evaluation functions

def train(model, loaders, config, add_hook_fn, hulls=None):
    """
    Train and evaluate model
    loaders : dict containing training and test 
    """

    model.cuda()

    ## setup optimizer
    if config.optimizer == 'sgd':
        print('Using SGD optimizer')
        opt = SGD(model.parameters(),
                  lr=config.lr,
                  momentum=config.momentum,
                  weight_decay=config.weight_decay)

    elif config.optimizer == 'adam':
        opt = AdamW(model.parameters(),
                    lr=config.lr,
                    weight_decay=config.weight_decay)

    else:
        raise NotImplementedError

    ## resume training
    if config.resume_step>0 and config.resume_dir is not None:

        assert os.path.exists(
            os.path.join(
                config.load_dir, f'checkpoint-s:{config.resume_step}.pt'
                )
            ), f"Resume checkpoint not found"

        base_chkpt = ch.load(os.path.join(config.resume_dir,
                             f'checkpoint-s:{-1}.pt'))
        model = base_chkpt['model']
        opt = base_chkpt['optimizer']
        state_chkpt = ch.load(os.path.join(config.resume_dir,
                              f'checkpoint-s:{config.resume_step}.pt'))
        model.load_state_dict(state_chkpt['model_state_dict'])
        opt.load_state_dict(state_chkpt['optimizer_state_dict'])


    ### save model and optimizer before training
    ### shortcut to avoid removing hooks later
    ch.save(
              {
                  'model': model,
                  'optimizer': opt,
              },
              os.path.join(
                  config.model_dir,
                  f'checkpoint-s:{-1}.pt'
              )
    )

    iters_per_epoch = len(loaders['train'])
    epochs = np.floor(config.num_steps/iters_per_epoch)

    if config.lr_schedule_flag:
        # Cyclic LR with single triangle
        print('Using Learning Rate Schedule')
        lr_schedule = np.interp(np.arange((epochs+1) * iters_per_epoch),
                                [0, config.lr_peak_epoch * iters_per_epoch, epochs * iters_per_epoch],
                                [0, 1, 0])

        scheduler = lr_scheduler.LambdaLR(opt, lr_schedule.__getitem__)

    loss_fn = CrossEntropyLoss(
        label_smoothing=config.label_smoothing
        )


    train_step = 0 if config.resume_step <= 0 else config.resume_step

    ## stat dict for plotting convenience
    stat_names = ['train_acc','train_loss','test_loss',
                  'test_acc','adv_acc', 'train_step'] + \
            [each+'_LC' for each in list(hulls.keys())]

    stats = dict(zip(stat_names,[[] for _ in stat_names]))

    print(f'Logging stats for steps:{config.log_steps}')

    while True:

        if train_step > config.num_steps: break

        for ims, labs in tqdm(loaders['train'], desc=f"train_step:{train_step}-{train_step+iters_per_epoch}"):

            ### train step
            ims = ims.cuda()
            labs = labs.cuda()

            if config.use_ffcv and labs.max()>config.num_class-1:
              labs = ch.clip(labs,0,config.num_class-1) ## weird ffcv bug

            opt.zero_grad()
            out = model(ims)
            loss = loss_fn(out, labs)
            loss.backward()
            opt.step()
            train_step += 1

            if config.lr_schedule_flag:
                scheduler.step()

            ### log step
            if train_step in config.log_steps:
                print('Computing stats...')

                model.eval()

                ### checkpoint before anything
                if config.save_model:

                  ch.save(
                      {
                          'model_state_dict': model.state_dict(),
                          'optimizer_state_dict': opt.state_dict(),
                      },
                      os.path.join(
                          config.model_dir,
                          f'checkpoint-s:{train_step}.pt'
                      )
                  )

                ## evaluate on train and test
                train_acc, train_loss = evaluate(model,
                                                 loaders['train'],
                                                 loss_fn)
                test_acc, test_loss = evaluate(model,loaders['test'],
                                                 loss_fn)

                stats['train_acc'].append(train_acc)
                stats['test_acc'].append(test_acc)
                stats['train_loss'].append(train_loss)
                stats['test_loss'].append(test_loss)
                stats['train_step'].append(train_step)

                ## evaluate local complexity
                # add hooks
                if config.compute_LC:

                  model, layer_names, activation_buffer = add_hook_fn(model, config)

                  if hulls is not None:
                    for k in hulls.keys():

                      # compute number of neurons that intersect hulls
                      # using network activations

                      with ch.no_grad():

                        n_inters, _ = get_intersections_for_hulls(
                                        hulls[k],
                                        model=model,
                                        batch_size=config.LC_batch_size,
                                        layer_names=layer_names,
                                        activation_buffer=activation_buffer
                                  )

                      stats[k+'_LC'].append(n_inters.cpu())

                ## evaluate robustness
                if config.compute_robust:

                  adv_acc = evaluate_adv(model, loaders['test'], config)
                  stats['adv_acc'].append(adv_acc)


                if config.wandb_log:
                  wandb.log({
                      'iter' : train_step,
                      'train/acc': stats['train_acc'][-1],
                      'train/loss': stats['train_loss'][-1],
                      'test/acc' : stats['test_acc'][-1],
                      'test/loss' : stats['test_loss'][-1],
                      'train/LC' : stats['train_LC'][-1].sum(1).mean(0),
                      'test/LC' : stats['test_LC'][-1].sum(1).mean(0),
                      'random/LC' : stats['rand_LC'][-1].sum(1).mean(0),
                      'adv/acc' : stats['adv_acc'][-1]
                  })

                model.train()


    ## save after training is complete
    ch.save(
        {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': opt.state_dict(),
        },
        os.path.join(
            config.model_dir,
            f'checkpoint-s:{train_step}.pt'
        )
    )

    return stats

@ch.no_grad
def evaluate(model, dloader, loss_fn=None):

  acc = 0
  loss = 0
  nsamples = 0
  nbatch = 0

  for ims, labs in dloader:

      ims = ims.cuda()
      labs = labs.cuda()

      outs = model(ims)

      if loss_fn is not None:
        loss += loss_fn(outs, labs)
        nbatch += 1

      acc += ch.sum(labs == outs.argmax(dim=-1)).cpu()
      nsamples += outs.shape[0]

  return acc/nsamples, loss/nbatch

def evaluate_adv(model, dloader, config):

  atk = PGD(model,
          eps=config.atk_eps,
          alpha=config.atk_alpha,
          steps=config.atk_itrs,
          dmin=config.dmin,
          dmax=config.dmax
          )

  acc = 0
  nsamples = 0
  for ims, labs in tqdm(dloader, desc=f"Computing robust acc for eps:{config.atk_eps:.3f}"):

    ims = ims.cuda()
    labs = labs.cuda()

    adv_images = atk(ims, labs)

    with ch.no_grad():
        adv_pred = model(adv_images).argmax(dim=-1)

    acc += ch.sum(labs == adv_pred).cpu()
    nsamples += adv_pred.shape[0]

  return acc/nsamples

if __name__ == '__main__':
    
  config = config_resnet18_cifar10()

  ## load data
  if not config.use_ffcv:
    train_loader, test_loader = cifar10_dataloaders(config)
  else:
    train_loader, test_loader = cifar10_dataloaders_ffcv(config,
                                                       num_workers=-1)

## initialize neighborhood sampler
  sampler_params = {'n' : config.n_frame if not config.inc_centroid \
                    else config.n_frame+1,
                  'r' : config.r_frame, 'seed':config.seed}

  sampler = get_ortho_hull_around_samples_w_orig if config.inc_centroid \
            else get_ortho_hull_around_samples

## select samples for neighborhood computation
  train_LC_batch, _ = get_LC_samples(train_loader,config)
  test_LC_batch, _ = get_LC_samples(train_loader,config)
  if config.normalize:
    rand_LC_batch = ch.rand_like(test_LC_batch
                                )*(config.dmax-config.dmin) + config.dmin
  else:
    rand_LC_batch = ch.rand_like(test_LC_batch)   ## Data domain [0,1]


## sample hulls/neighborhoods
  train_hulls = sampler(
    train_LC_batch.cuda(),
    **sampler_params
  ).cpu()
  test_hulls = sampler(
      test_LC_batch.cuda(),
      **sampler_params
  ).cpu()
  rand_hulls = sampler(
      rand_LC_batch.cuda(),
      **sampler_params
  ).cpu()


  ## make hull dict. the keys of this dict will be used for logging
  ## you can add hulls separately for different classes as well to
  ## keep track of classwise statistics
  hulls = {
    'train' : train_hulls,
    'test' : test_hulls,
    'rand' : rand_hulls
  }

  loaders = {
    'train' : train_loader,
    'test' : test_loader
  }

  ## directory for saving logs and models
  timestamp = time.ctime().replace(' ','_')
  config.model_dir = os.path.join(f'./models/{timestamp}')
  config.log_dir = os.path.join(f'./logs/{timestamp}')
  os.mkdir(config.model_dir)
  os.mkdir(config.log_dir)

  if config.wandb_log:
    wandb_project = config.wandb_proj
    wandb_run_name = f"{config.wandb_pref}-{timestamp}"
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)

  ## create model
  model = make_resnet18k(k=config.k,
                       num_classes=config.num_class,
                       bn=config.use_bn)

  stats = train(model, loaders,
      config=config, hulls=hulls,
      add_hook_fn=add_hooks_preact_resnet18
      )

  ch.save(
      stats,
      os.path.join(config.log_dir,'stats.pt')
  )